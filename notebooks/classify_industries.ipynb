{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify Industries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOS \n",
    "\n",
    "- remove pos einführen (siehe `clustering_whole_corpus`)\n",
    "    - vielleicht mit Language identifier?\n",
    "- preprocessing function einführen\n",
    "    - `\\n` weg\n",
    "    - andere unnütze Zeichen wie `|` etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDEEN\n",
    "\n",
    "### Datensatzaufbereitung\n",
    "\n",
    "- Übersetzung der Websites in einheitliche Sprache (z.b. Englisch)\n",
    "- Andere Klassenlabels?\n",
    "    - Erst allgemeinere Klassen und dann in diesen Klassen feiner klassifizieren?\n",
    "        - Von: https://towardsdatascience.com/industrial-classification-of-websites-by-machine-learning-with-hands-on-python-3761b1b530f1\n",
    "            - Technology, Office, & Education products website (Class_1)\n",
    "            - Consumer products website (Class_2)\n",
    "            - Industrial Tools and Hardware products website (Class_3)\n",
    "    - seltene Klassenlables wegwerfen?\n",
    "\n",
    "### HTML Klassifizierung\n",
    "\n",
    "\n",
    "- Text zusammenfassen und dann klassifizieren? Dafür auch HTML-Tags verwenden?\n",
    "\n",
    "- HTML Struktur verwenden, um vorher **Boilerplate Content** von Main Content zu entfernen:\n",
    "    - Plain Text ist sehr noisy (viel unnötiges drin)\n",
    "- Bestimmten Wörtern/Tags höhere Gewichtungen geben\n",
    "    - Anchor Text (= klickbarer Text in einem Hyperlink)\n",
    "        - alleine zu wenig Inhalt (QI, S. 12)\n",
    "        - umliegende Wörter interessant! (QI, S. 12)\n",
    "        - auch für Nachbar-Seiten-Ansatz\n",
    "    - Title, Headers (QI, S. 12)\n",
    "        - auch für Nachbar-Seiten-Ansatz\n",
    "    - Keywords für Branchen\n",
    "        ```python3  \n",
    "        Class_1_keywords = ['Office', 'School', 'phone', 'Technology', 'Electronics', 'Cell', 'Business', 'Education', 'Classroom']\n",
    "        \n",
    "        Class_2_keywords = ['Restaurant', 'Hospitality', 'Tub', 'Drain', 'Pool', 'Filtration', 'Floor', 'Restroom', 'Consumer', 'Care', 'Bags', 'Disposables']\n",
    "        \n",
    "        Class_3_keywords = ['Pull', 'Lifts', 'Pneumatic', 'Emergency', 'Finishing', 'Hydraulic', 'Lockout', 'Towers', 'Drywall', 'Tools', 'Packaging', 'Measure', 'Tag ']\n",
    "        ```\n",
    "- flat classification oder hierarchical classification?\n",
    "    - flat: parallele Klassen\n",
    "    - hierarchical: hierarchische Klassen, bauen aufeinander auf\n",
    "- Nur nach bestimmten Keywords filtern? (das geht jedoch mehr Richtung PLAIN-Textclassification)\n",
    "- \"implicit links\": Seiten, die beide bei Suche von **Suchmaschine** erschienen sind und auf die beide der User geklickt hat (QI, S. 12) &rarr; nicht wirklich realisierbar\n",
    "\n",
    "\n",
    "## Paper / Repos\n",
    "\n",
    "- **Boilerplate Removal using a Neural Sequence Labeling Model** (2020): https://arxiv.org/pdf/2004.14294.pdf\n",
    "    - Verbesserung von **Web2Text** &rarr; basiert nicht auf teuren, handgemachten Feature Engineering\n",
    "    - <u>Hypothese</u>: \"Our hypothesis is that the **order** of text blocks in a web page **encodes important information** about their type, i.e. content or boilerplate, as the placement is determined by the authoring style\"\n",
    "- **Web2Text: Deep Structured Boilerplate Removal** (2018): https://arxiv.org/pdf/1801.02607.pdf\n",
    "- **Mozillas readability**: https://github.com/mozilla/readability\n",
    "- **Webpage Classification based on Compound of Using HTML Features & URLFeatures and Features of Sibling Pages** (2010): https://www.researchgate.net/publication/220419545_Webpage_Classification_based_on_Compound_of_Using_HTML_Features_URL_Features_and_Features_of_Sibling_Pages\n",
    "    - TODO\n",
    "- **Web Page Classification: Features and Algorithms** (2009): https://www.cs.ucf.edu/~dcm/Teaching/COT4810-Fall%202012/Literature/WebPageClassification.pdf\n",
    "    - S. 7: Using On-Page Features\n",
    "        - GOLUB, ARDO (2005): title, headings, metadata, main text\n",
    "    - TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "Evaluation metric: **F1 Scores**\n",
    "\n",
    "| Experiment | Dummy | LSVM |\n",
    "| ---------- |:-----:| ----:|\n",
    "| Plain Text (kein POS Removal) | 0.01665 | 0.50609 |\n",
    "| Plain Text (POS Removal: VERB, ADJ, NOUN) | 0.01417 | 0.01395 |\n",
    "| HTML | 0.0145 | 0.46618 |\n",
    "| Clean HTML | 0.01554 | 0.39454 |\n",
    "| Plain Text (POS Removal: NOUN, only DE) | 0.01707 | 0.01865 |\n",
    "| HTML (only DE) | 0.01709 | 0.41037 |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# sklearn classification\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# sklearn clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# sklearn general\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "import ujson as json\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from app.utils import remove_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/\"\n",
    "\n",
    "INDUSTRY_CODES_PATH = DATA_PATH + \"linkedin-industry-codes.json\"\n",
    "TRAIN_PATH_JSON = DATA_PATH + \"train.ndjson\"\n",
    "TEST_PATH_JSON = DATA_PATH + \"test.ndjson\"\n",
    "TRAIN_PATH_CSV = DATA_PATH + \"train.csv\"\n",
    "TEST_PATH_CSV = DATA_PATH + \"test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM_RED = False\n",
    "MAX_DOCUMENT_FREQUENCY = 1.\n",
    "MAX_FEATURES = 1000000\n",
    "LOWERCASE = False\n",
    "STOP_WORDS = get_stop_words(\"de\")\n",
    "\n",
    "# POS TAGGING\n",
    "POS_TAGGING = True\n",
    "POS_TAGS = [\"NOUN\"]\n",
    "\n",
    "# SUBSAMPLING\n",
    "SUBSAMPLING = True\n",
    "N_SAMPLES = 10000\n",
    "USED_LANG = [\"DE\"] # \"ALL\" for no removal\n",
    "\n",
    "\n",
    "# HTML\n",
    "USE_HTML = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load train csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.36 s, sys: 889 ms, total: 9.25 s\n",
      "Wall time: 9.25 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>html</th>\n",
       "      <th>industry</th>\n",
       "      <th>country</th>\n",
       "      <th>industry_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Home | NETZkultur GmbH\\n\\nZum Inhalt wechseln\\...</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html lang=\"de-DE\"&gt;\\n&lt;head&gt;\\n...</td>\n",
       "      <td>4</td>\n",
       "      <td>DE</td>\n",
       "      <td>Computer Software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\nNXP Semiconductors | Automotive, Security,...</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html&gt;\\n&lt;head&gt;\\n\\t&lt;title&gt;NXP ...</td>\n",
       "      <td>7</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>Semiconductors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Suer Nutzfahrzeugtechnik Onlineshop\\n\\nSie wis...</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html lang=\"de\"&gt;\\n    &lt;head&gt;\\...</td>\n",
       "      <td>53</td>\n",
       "      <td>DE</td>\n",
       "      <td>Automotive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Improve cash flows and long-term profitability...</td>\n",
       "      <td>\\n&lt;!DOCTYPE html&gt;\\n&lt;html lang=\"en\" prefix=\"og:...</td>\n",
       "      <td>43</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>Financial Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Your specialist for plastic compounds\\n\\nMenu ...</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html xmlns:og=\"http://ogp.me...</td>\n",
       "      <td>117</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>Plastics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Home | NETZkultur GmbH\\n\\nZum Inhalt wechseln\\...   \n",
       "1  \\n\\nNXP Semiconductors | Automotive, Security,...   \n",
       "2  Suer Nutzfahrzeugtechnik Onlineshop\\n\\nSie wis...   \n",
       "3  Improve cash flows and long-term profitability...   \n",
       "4  Your specialist for plastic compounds\\n\\nMenu ...   \n",
       "\n",
       "                                                html  industry  country  \\\n",
       "0  <!DOCTYPE html>\\n<html lang=\"de-DE\">\\n<head>\\n...         4       DE   \n",
       "1  <!DOCTYPE html>\\n<html>\\n<head>\\n\\t<title>NXP ...         7  UNKNOWN   \n",
       "2  <!DOCTYPE html>\\n<html lang=\"de\">\\n    <head>\\...        53       DE   \n",
       "3  \\n<!DOCTYPE html>\\n<html lang=\"en\" prefix=\"og:...        43  UNKNOWN   \n",
       "4  <!DOCTYPE html>\\n<html xmlns:og=\"http://ogp.me...       117  UNKNOWN   \n",
       "\n",
       "        industry_name  \n",
       "0   Computer Software  \n",
       "1      Semiconductors  \n",
       "2          Automotive  \n",
       "3  Financial Services  \n",
       "4            Plastics  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "train = pd.read_csv(TRAIN_PATH_CSV)\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13114, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apparel & Fashion',\n",
       " 'Architecture & Planning',\n",
       " 'Automotive',\n",
       " 'Banking',\n",
       " 'Building Materials',\n",
       " 'Chemicals',\n",
       " 'Civil Engineering',\n",
       " 'Computer & Network Security',\n",
       " 'Computer Hardware',\n",
       " 'Computer Software',\n",
       " 'Construction',\n",
       " 'Consumer Goods',\n",
       " 'Electrical/Electronic Manufacturing',\n",
       " 'Events Services',\n",
       " 'Financial Services',\n",
       " 'Food & Beverages',\n",
       " 'Furniture',\n",
       " 'Government Administration',\n",
       " 'Graphic Design',\n",
       " 'Health, Wellness and Fitness',\n",
       " 'Higher Education',\n",
       " 'Human Resources',\n",
       " 'Industrial Automation',\n",
       " 'Information Technology and Services',\n",
       " 'Insurance',\n",
       " 'Internet',\n",
       " 'Legal Services',\n",
       " 'Leisure, Travel & Tourism',\n",
       " 'Logistics and Supply Chain',\n",
       " 'Management Consulting',\n",
       " 'Marketing and Advertising',\n",
       " 'Mechanical or Industrial Engineering',\n",
       " 'Medical Devices',\n",
       " 'Mining & Metals',\n",
       " 'Oil & Energy',\n",
       " 'Online Media',\n",
       " 'Pharmaceuticals',\n",
       " 'Plastics',\n",
       " 'Printing',\n",
       " 'Professional Training & Coaching',\n",
       " 'Public Relations and Communications',\n",
       " 'Publishing',\n",
       " 'Real Estate',\n",
       " 'Renewables & Environment',\n",
       " 'Retail',\n",
       " 'Semiconductors',\n",
       " 'Telecommunications',\n",
       " 'Transportation/Trucking/Railroad',\n",
       " 'Utilities',\n",
       " 'Wholesale']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(np.unique(train.industry_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsampling\n",
    "\n",
    "- Only specific language (e.g. \"DE\")\n",
    "- Only first $n$ samples (e.g. 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7375, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if SUBSAMPLING:\n",
    "    \n",
    "    if USED_LANG[0] != \"ALL\":\n",
    "        train = train[train.country.isin(USED_LANG)]\n",
    "    train = train.head(N_SAMPLES)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing (vectorizing, dimension reducing etc.)\n",
    "\n",
    "- ignore terms with a document frequency > MAX_DOCUMENT_FREQUENCY (`max_df` in TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of unique industry names in train set: 50\n",
      "Count of unique languages in train set: 1\n"
     ]
    }
   ],
   "source": [
    "if USE_HTML:\n",
    "    POS_TAGGING = False\n",
    "    train_text_plain = train[\"html\"].values\n",
    "else:\n",
    "    train_text_plain = train[\"text\"].values\n",
    "\n",
    "\n",
    "train_labels = train[\"industry\"].values\n",
    "unique_train_labels = list(np.unique(train[\"industry\"]))\n",
    "print(\"Count of unique industry names in train set:\", len(unique_train_labels))\n",
    "print(\"Count of unique languages in train set:\", len(np.unique(train[\"country\"].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No POS TAGS are removed.\n",
      "\n",
      "CPU times: user 90 µs, sys: 37 µs, total: 127 µs\n",
      "Wall time: 111 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if POS_TAGGING:\n",
    "    train_text = remove_pos(train, pos_tags=POS_TAGS)\n",
    "else:\n",
    "    train_text = train_text_plain\n",
    "    print(\"No POS TAGS are removed.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 32s, sys: 389 ms, total: 1min 32s\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=MAX_DOCUMENT_FREQUENCY,\n",
    "                             lowercase=LOWERCASE,\n",
    "                             max_features=MAX_FEATURES,\n",
    "                             stop_words=STOP_WORDS)\n",
    "\n",
    "\n",
    "vectorizer.fit(train_text)\n",
    "\n",
    "train_vector = vectorizer.transform(train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Dataset\n",
    "\n",
    "There is one class/industry which appears in test set but not in the training set. All instances of this class were removed from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.21 s, sys: 261 ms, total: 3.47 s\n",
      "Wall time: 3.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test = pd.read_csv(TEST_PATH_CSV)\n",
    "test = test[test[\"industry\"].isin(unique_train_labels)]\n",
    "\n",
    "if SUBSAMPLING:\n",
    "    test = test[test.country.isin(USED_LANG)]\n",
    "\n",
    "\n",
    "test_vector = vectorizer.transform(test[\"text\"].values)\n",
    "test_labels = test[\"industry\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get industry names for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(INDUSTRY_CODES_PATH) as f:\n",
    "    industry_codes = json.load(f)\n",
    "    \n",
    "def get_code(code_list, identifier):\n",
    "    name = \"\"\n",
    "    for entry in code_list:\n",
    "        if entry[\"Code\"] == identifier:\n",
    "            name = entry[\"Description\"]\n",
    "            break\n",
    "    return name\n",
    "\n",
    "test_label_names = list(map(lambda x: get_code(industry_codes, x), dict(test[\"industry\"]).values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy CLF \n",
      "-------------------------\n",
      "0.02184 \tPrecision\n",
      "0.023 \tRecall\n",
      "0.01709 \tF1\n",
      "\n",
      "CPU times: user 11.1 ms, sys: 1.3 ms, total: 12.4 ms\n",
      "Wall time: 11.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Dummy CLF\", \"\\n-------------------------\")\n",
    "clf = DummyClassifier(strategy=\"uniform\")\n",
    "clf.fit(train_vector, train_labels)\n",
    "\n",
    "train_preds = clf.predict(test_vector)\n",
    "\n",
    "precision = precision_score(test_labels, train_preds, average=\"macro\")\n",
    "recall = recall_score(test_labels, train_preds, average=\"macro\")\n",
    "f1 = f1_score(test_labels, train_preds, average=\"macro\")\n",
    "print(np.round(precision, decimals=5), \"\\tPrecision\")\n",
    "print(np.round(recall, decimals=5), \"\\tRecall\")\n",
    "print(np.round(f1, decimals=5), \"\\tF1\")\n",
    "print()\n",
    "\n",
    "clf_report = classification_report(test_labels, train_preds, target_names = np.unique(test_label_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSVM CLF \n",
      "-------------------------\n",
      "0.56993 \tPrecision\n",
      "0.36637 \tRecall\n",
      "0.41037 \tF1\n",
      "\n",
      "CPU times: user 42.1 s, sys: 182 ms, total: 42.3 s\n",
      "Wall time: 42.3 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jan/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jan/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LinearSVC(C = 1)\n",
    "clf.fit(train_vector, train_labels)\n",
    "\n",
    "train_preds = clf.predict(test_vector)\n",
    "\n",
    "# Metrics\n",
    "print(\"LSVM CLF\", \"\\n-------------------------\")\n",
    "precision = precision_score(test_labels, train_preds, average=\"macro\")\n",
    "recall = recall_score(test_labels, train_preds, average=\"macro\")\n",
    "f1 = f1_score(test_labels, train_preds, average=\"macro\")\n",
    "print(np.round(precision, decimals=5), \"\\tPrecision\")\n",
    "print(np.round(recall, decimals=5), \"\\tRecall\")\n",
    "print(np.round(f1, decimals=5), \"\\tF1\")\n",
    "print()\n",
    "\n",
    "clf_report = classification_report(test_labels, train_preds, target_names = np.unique(test_label_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMALIZE_CM = True\n",
    "INDUSTRY_TRESHOLD = 250\n",
    "PLT_SCALING_FACTOR = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "filtered_train = train.groupby(\"industry_name\").filter(lambda x: len(x)>INDUSTRY_TRESHOLD)\n",
    "remaining_industries = filtered_train[\"industry_name\"].drop_duplicates().tolist()\n",
    "\n",
    "\n",
    "\n",
    "cnf_matrix = confusion_matrix(test_labels, train_preds)\n",
    "\n",
    "classes = train[\"industry_name\"].drop_duplicates().tolist()\n",
    "\n",
    "cnf_df = pd.DataFrame(cnf_matrix, index=classes, columns=classes)\n",
    "cnf_df = cnf_df[remaining_industries]\n",
    "cnf_df = cnf_df.loc[remaining_industries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10*PLT_SCALING_FACTOR, 8*PLT_SCALING_FACTOR))\n",
    "\n",
    "if NORMALIZE_CM:\n",
    "    normalized_cnf_df = cnf_df.astype('float') / cnf_df.sum(axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(normalized_cnf_df, annot=True, cmap=sns.color_palette(\"Blues\"), fmt='.2f')\n",
    "else:\n",
    "    sns.heatmap(cnf_df, annot=True, cmap=sns.color_palette(\"Blues\"), fmt='g')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
