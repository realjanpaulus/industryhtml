{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify Industries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOS \n",
    "\n",
    "- top 10 klassen oder so\n",
    "- preprocessing function einführen\n",
    "    - `\\n` weg\n",
    "    - andere unnütze Zeichen wie `|` etc.\n",
    "- remove pos einführen (siehe `clustering_whole_corpus`)\n",
    "    - vielleicht mit Language identifier?\n",
    "- ~~\"country\" als colum~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDEEN\n",
    "\n",
    "### Datensatzaufbereitung\n",
    "\n",
    "- Übersetzung der Websites in einheitliche Sprache (z.b. Englisch)\n",
    "- Andere Klassenlabels?\n",
    "    - Erst allgemeinere Klassen und dann in diesen Klassen feiner klassifizieren?\n",
    "        - Von: https://towardsdatascience.com/industrial-classification-of-websites-by-machine-learning-with-hands-on-python-3761b1b530f1\n",
    "            - Technology, Office, & Education products website (Class_1)\n",
    "            - Consumer products website (Class_2)\n",
    "            - Industrial Tools and Hardware products website (Class_3)\n",
    "    - seltene Klassenlables wegwerfen?\n",
    "\n",
    "### HTML Klassifizierung\n",
    "\n",
    "\n",
    "- Text zusammenfassen und dann klassifizieren? Dafür auch HTML-Tags verwenden?\n",
    "- ~~HTML Struktur verwenden, um vorher **Boilerplate Content** von Main Content zu entfernen:~~\n",
    "    - ~~Plain Text ist sehr noisy (viel unnötiges drin)~~\n",
    "    - ICH: gemacht mit CLEAN HTML, aber ohne explizites Boilerplate Content Removal\n",
    "- Bestimmten Wörtern/Tags höhere Gewichtungen geben\n",
    "    - Anchor Text (= klickbarer Text in einem Hyperlink)\n",
    "        - alleine zu wenig Inhalt (QI, S. 12)\n",
    "        - umliegende Wörter interessant! (QI, S. 12)\n",
    "        - auch für Nachbar-Seiten-Ansatz\n",
    "    - Title, Headers (QI, S. 12)\n",
    "        - auch für Nachbar-Seiten-Ansatz\n",
    "    - Keywords für Branchen\n",
    "        ```python3  \n",
    "        Class_1_keywords = ['Office', 'School', 'phone', 'Technology', 'Electronics', 'Cell', 'Business', 'Education', 'Classroom']\n",
    "        \n",
    "        Class_2_keywords = ['Restaurant', 'Hospitality', 'Tub', 'Drain', 'Pool', 'Filtration', 'Floor', 'Restroom', 'Consumer', 'Care', 'Bags', 'Disposables']\n",
    "        \n",
    "        Class_3_keywords = ['Pull', 'Lifts', 'Pneumatic', 'Emergency', 'Finishing', 'Hydraulic', 'Lockout', 'Towers', 'Drywall', 'Tools', 'Packaging', 'Measure', 'Tag ']\n",
    "        ```\n",
    "- NER mit **Tags** als weitere Tokens\n",
    "- Features von \"Nachbarseiten\" verwenden\n",
    "    - Hilfreich, da mehr Infos als \"nur\" Startseite\n",
    "    - Fragen: \n",
    "        - Was sind Nachbarseiten, wie definieren?\n",
    "            - Webgraph Webseiten?\n",
    "            - Weitere Seiten des Unternehmens?\n",
    "        - Wie viele Nachbarseiten?\n",
    "        - Wieviel von den Nachbarseiten verwenden?\n",
    "            - Ganze Seite?\n",
    "            - text, title, heading, Metadaten?\n",
    "    \n",
    "- *Weiteres*:\n",
    "    - Flat classification oder Hierarchical classification?\n",
    "        - Flat: parallele Klassen\n",
    "        - Hierarchical: hierarchische Klassen, bauen aufeinander auf\n",
    "    - Nur nach bestimmten Keywords filtern? (das geht jedoch mehr Richtung PLAIN-Textclassification)\n",
    "    - \"implicit links\": Seiten, die beide bei Suche von **Suchmaschine** erschienen sind und auf die beide der User geklickt hat (QI, S. 12) &rarr; nicht wirklich realisierbar\n",
    "\n",
    "\n",
    "## Paper / Repos\n",
    "\n",
    "- **Boilerplate Removal using a Neural Sequence Labeling Model** (2020): https://arxiv.org/pdf/2004.14294.pdf\n",
    "    - Verbesserung von **Web2Text** &rarr; basiert nicht auf teuren, handgemachten Feature Engineering\n",
    "    - <u>Hypothese</u>: \"Our hypothesis is that the **order** of text blocks in a web page **encodes important information** about their type, i.e. content or boilerplate, as the placement is determined by the authoring style\"\n",
    "- **Web2Text: Deep Structured Boilerplate Removal** (2018): https://arxiv.org/pdf/1801.02607.pdf\n",
    "- **Mozillas readability**: https://github.com/mozilla/readability\n",
    "- **Webpage Classification based on Compound of Using HTML Features & URLFeatures and Features of Sibling Pages** (2010): https://www.researchgate.net/publication/220419545_Webpage_Classification_based_on_Compound_of_Using_HTML_Features_URL_Features_and_Features_of_Sibling_Pages\n",
    "    - TODO\n",
    "- **Web Page Classification: Features and Algorithms** (2009): https://www.cs.ucf.edu/~dcm/Teaching/COT4810-Fall%202012/Literature/WebPageClassification.pdf\n",
    "    - S. 7: Using On-Page Features\n",
    "        - GOLUB, ARDO (2005): title, headings, metadata, main text\n",
    "    - TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "- Evaluation metric: **F1 Scores**\n",
    "- TF-IDF Vectorizer\n",
    "    - kein lowercase\n",
    "    - stop words werden entfernt\n",
    "    - keine max features\n",
    "- Top $n$ classes = most frequent classes\n",
    "- CLEAN HTML auch für Test Set (ansonsten unglaublich schlechte Accuracy und etwas sinnlos)\n",
    "\n",
    "\n",
    "#### Label: `group_representatives`\n",
    "\n",
    "| Experiment | SGD F1 (Precision) | LSVM F1 (Precision) |\n",
    "| ---------- |:-----:| ----:|\n",
    "| HTML (10000 features) | **0.5292** (0.5962) | **0.5493** (0.6371) |\n",
    "| HTML (10000 features) ((1, 3) ngrams) | **0.4035** (0.463) | **0.4188** (0.5345) |\n",
    "| HTML (10000 features) ((2, 2) ngrams) | **0.2442** (0.2787) | **0.252** (0.3146) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aktuelle TODOS zum ausprobieren (26.04)\n",
    "- N-gramme &rarr; erstmal lassen\n",
    "- Max Features\n",
    "- Count Vectorizer / TF-IDF Vectorizer manipulieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# sklearn classification\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# sklearn general\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "import ujson as json\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from app.utils import clean_html_boilerplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR_PATH = \"../data/\"\n",
    "LANG = \"_DE\"\n",
    "\n",
    "INDUSTRIES_PATH_CSV = DATA_DIR_PATH + \"industries.csv\"\n",
    "TRAIN_PATH_CSV = DATA_DIR_PATH + \"train\" + LANG + \".csv\"\n",
    "TEST_PATH_CSV = DATA_DIR_PATH + \"test\" + LANG + \".csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.33 s, sys: 1.08 s, total: 7.41 s\n",
      "Wall time: 7.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = pd.read_csv(TRAIN_PATH_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>industry</th>\n",
       "      <th>industry_label</th>\n",
       "      <th>group</th>\n",
       "      <th>group_representative</th>\n",
       "      <th>html</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>country</th>\n",
       "      <th>group_representative_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.blume-rollen.com</td>\n",
       "      <td>135</td>\n",
       "      <td>Mechanical or Industrial Engineering</td>\n",
       "      <td>cons, gov, man</td>\n",
       "      <td>135</td>\n",
       "      <td>&lt;html class=\"no-js\" lang=\"de\"&gt; &lt;head&gt;         ...</td>\n",
       "      <td>BLUME FÖRDERANLAGEN | BLUME-ROLLEN GMBH - FÖRD...</td>\n",
       "      <td>xing</td>\n",
       "      <td>DE</td>\n",
       "      <td>Mechanical or Industrial Engineering</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           url  industry  \\\n",
       "0  http://www.blume-rollen.com       135   \n",
       "\n",
       "                         industry_label           group  group_representative  \\\n",
       "0  Mechanical or Industrial Engineering  cons, gov, man                   135   \n",
       "\n",
       "                                                html  \\\n",
       "0  <html class=\"no-js\" lang=\"de\"> <head>         ...   \n",
       "\n",
       "                                                text source country  \\\n",
       "0  BLUME FÖRDERANLAGEN | BLUME-ROLLEN GMBH - FÖRD...   xing      DE   \n",
       "\n",
       "             group_representative_label  \n",
       "0  Mechanical or Industrial Engineering  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19857, 10)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"text\" or \"html\"\n",
    "TEXT_COL = \"html\"\n",
    "\n",
    "# \"group_representative\", \"group_representative_label\", \"industry\", \"industry_label\" or \"group\"\n",
    "CLASS_COL = \"group_representative\"\n",
    "CLASS_NAMES = \"group_representative_label\"\n",
    "\n",
    "MAX_DOCUMENT_FREQUENCY = 1.\n",
    "MAX_FEATURES = 100\n",
    "NGRAM_RANGE = (1,1)\n",
    "LOWERCASE = False\n",
    "STOP_WORDS = get_stop_words(\"de\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: eigener vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.head() # todo weg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.4 ms, sys: 100 µs, total: 25.5 ms\n",
      "Wall time: 31.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_text = train[TEXT_COL]\n",
    "train_labels = train[CLASS_COL].values\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=MAX_DOCUMENT_FREQUENCY,\n",
    "                             lowercase=LOWERCASE,\n",
    "                             max_features=MAX_FEATURES,\n",
    "                             ngram_range=NGRAM_RANGE,\n",
    "                             stop_words=STOP_WORDS)\n",
    "transformer = TfidfTransformer()\n",
    "\n",
    "vector = vectorizer.fit_transform(train_text)\n",
    "train_vector = transformer.fit_transform(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- building own preprocessor for html tags\n",
    "- vielleicht auch eigenen tokenizer der html tags als einzelne tokens ansieht\n",
    "    - https://stackoverflow.com/questions/47549856/tokenizing-an-html-document\n",
    "    - hier vielleicht auch gucken, welche tokens man nur verwenden möchte\n",
    "    - vorher klassen und sowas entfernen? ja oder, da sich (wahrscheinlich) nur auf hauseigene css klassen und sowas bezieht, ist ja eher unwichtig\n",
    "    -  d.h. sowas schreiben wie `TagCleaner` oder so. mal bei lxml gucken &rarr; https://lxml.de/api/lxml.html.clean.Cleaner-class.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sie',\n",
       " '_blank',\n",
       " 'amp',\n",
       " 'anlegen',\n",
       " 'antrieb',\n",
       " 'background',\n",
       " 'block',\n",
       " 'blume',\n",
       " 'box',\n",
       " 'br',\n",
       " 'button',\n",
       " 'carousel',\n",
       " 'cf',\n",
       " 'class',\n",
       " 'col',\n",
       " 'col10',\n",
       " 'com',\n",
       " 'component',\n",
       " 'container',\n",
       " 'content',\n",
       " 'de',\n",
       " 'div',\n",
       " 'dropdown',\n",
       " 'dropdown__contact',\n",
       " 'dropdown__link',\n",
       " 'dropdown__title',\n",
       " 'entry',\n",
       " 'et',\n",
       " 'et_pb_bg_layout_light',\n",
       " 'et_pb_column',\n",
       " 'et_pb_css_mix_blend_mode_passthrough',\n",
       " 'et_pb_module',\n",
       " 'et_pb_text',\n",
       " 'et_pb_text_align_left',\n",
       " 'et_pb_text_inner',\n",
       " 'fa',\n",
       " 'foerderelemente',\n",
       " 'footer',\n",
       " 'gem',\n",
       " 'george',\n",
       " 'h2',\n",
       " 'h3',\n",
       " 'h4',\n",
       " 'header',\n",
       " 'href',\n",
       " 'https',\n",
       " 'icon',\n",
       " 'id',\n",
       " 'image',\n",
       " 'initialized',\n",
       " 'inner',\n",
       " 'item',\n",
       " 'karten',\n",
       " 'keller',\n",
       " 'konto',\n",
       " 'label',\n",
       " 'left',\n",
       " 'level',\n",
       " 'lg',\n",
       " 'li',\n",
       " 'link',\n",
       " 'list',\n",
       " 'md',\n",
       " 'menu',\n",
       " 'mobile',\n",
       " 'module',\n",
       " 'nav',\n",
       " 'navigation',\n",
       " 'noe',\n",
       " 'none',\n",
       " 'not',\n",
       " 'object',\n",
       " 'page',\n",
       " 'post_type',\n",
       " 'privatkunden',\n",
       " 'produkte',\n",
       " 'qbiq',\n",
       " 'qbiq_liste_01',\n",
       " 'reinhold',\n",
       " 'right',\n",
       " 'rollen',\n",
       " 'section',\n",
       " 'services',\n",
       " 'site',\n",
       " 'sm',\n",
       " 'spacer',\n",
       " 'span',\n",
       " 'sparen',\n",
       " 'strong',\n",
       " 'tabindex',\n",
       " 'taptap',\n",
       " 'target',\n",
       " 'text',\n",
       " 'tile',\n",
       " 'title',\n",
       " 'top',\n",
       " 'type',\n",
       " 'ul',\n",
       " 'wrapper',\n",
       " 'www']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.6 s, sys: 139 ms, total: 14.7 s\n",
      "Wall time: 14.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test = pd.read_csv(TEST_PATH_CSV)\n",
    "    \n",
    "\n",
    "test_vector = vectorizer.transform(test[TEXT_COL].values)\n",
    "test_labels = test[CLASS_COL].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD CLF \n",
      "-------------------------\n",
      "0.0145 \tPrecision\n",
      "0.057 \tRecall\n",
      "0.0211 \tF1\n",
      "\n",
      "CPU times: user 36.8 ms, sys: 714 µs, total: 37.5 ms\n",
      "Wall time: 70.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"SGD CLF\", \"\\n-------------------------\")\n",
    "# training\n",
    "clf = SGDClassifier()\n",
    "clf.fit(train_vector, train_labels)\n",
    "\n",
    "# prediction\n",
    "train_preds = clf.predict(test_vector)\n",
    "\n",
    "# evaluation\n",
    "precision = precision_score(test_labels, train_preds, average=\"macro\", zero_division=0)\n",
    "recall = recall_score(test_labels, train_preds, average=\"macro\", zero_division=0)\n",
    "f1 = f1_score(test_labels, train_preds, average=\"macro\", zero_division=0)\n",
    "clf1_f1 = np.round(f1, decimals=4)\n",
    "clf1_precision = np.round(precision, decimals=4)\n",
    "\n",
    "print(np.round(precision, decimals=4), \"\\tPrecision\")\n",
    "print(np.round(recall, decimals=4), \"\\tRecall\")\n",
    "print(np.round(f1, decimals=4), \"\\tF1\")\n",
    "print()\n",
    "\n",
    "clf1_report = classification_report(test_labels, \n",
    "                                   train_preds, \n",
    "                                   target_names = np.unique(test[CLASS_NAMES]), \n",
    "                                   zero_division = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSVM CLF \n",
      "-------------------------\n",
      "0.0212 \tPrecision\n",
      "0.0471 \tRecall\n",
      "0.0138 \tF1\n",
      "\n",
      "CPU times: user 29 ms, sys: 168 µs, total: 29.2 ms\n",
      "Wall time: 65.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"LSVM CLF\", \"\\n-------------------------\")\n",
    "# training\n",
    "clf = LinearSVC()\n",
    "clf.fit(train_vector, train_labels)\n",
    "\n",
    "# prediction\n",
    "train_preds = clf.predict(test_vector)\n",
    "\n",
    "# evaluation\n",
    "precision = precision_score(test_labels, train_preds, average=\"macro\", zero_division=0)\n",
    "recall = recall_score(test_labels, train_preds, average=\"macro\", zero_division=0)\n",
    "f1 = f1_score(test_labels, train_preds, average=\"macro\", zero_division=0)\n",
    "clf2_f1 = np.round(f1, decimals=4)\n",
    "clf2_precision = np.round(precision, decimals=4)\n",
    "\n",
    "print(np.round(precision, decimals=4), \"\\tPrecision\")\n",
    "print(np.round(recall, decimals=4), \"\\tRecall\")\n",
    "print(np.round(f1, decimals=4), \"\\tF1\")\n",
    "print()\n",
    "\n",
    "clf2_report = classification_report(test_labels, \n",
    "                                   train_preds, \n",
    "                                   target_names = np.unique(test[CLASS_NAMES]),\n",
    "                                   zero_division = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Classification Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_representative\n",
      "\n",
      "| HTML (100 features) | **0.0211** (0.0145) | **0.0138** (0.0212) |\n"
     ]
    }
   ],
   "source": [
    "result = \"| \"\n",
    "\n",
    "if TEXT_COL == \"text\":\n",
    "    result += \"Plain Text\"\n",
    "else:\n",
    "    result += \"HTML\"\n",
    "    \n",
    "if MAX_FEATURES is None:\n",
    "    result += \" (all features)\"\n",
    "else:\n",
    "    result += f\" ({MAX_FEATURES} features)\"\n",
    "    \n",
    "if NGRAM_RANGE != (1,1):\n",
    "    result += f\" ({NGRAM_RANGE} ngrams)\"\n",
    "    \n",
    "            \n",
    "result += f\" | **{clf1_f1}** ({clf1_precision}) | **{clf2_f1}** ({clf2_precision}) |\"\n",
    "print(CLASS_COL)\n",
    "print()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: label und text names und so; allg. änderungen von oben hier ergänzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMALIZE_CM = True\n",
    "INDUSTRY_TRESHOLD = 250\n",
    "PLT_SCALING_FACTOR = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "filtered_train = train.groupby(CLASS_COL).filter(lambda x: len(x)>INDUSTRY_TRESHOLD)\n",
    "remaining_industries = filtered_train[CLASS_NAMES].drop_duplicates().tolist()\n",
    "\n",
    "\n",
    "cnf_matrix = confusion_matrix(test_labels, train_preds)\n",
    "\n",
    "classes = train[CLASS_COL].drop_duplicates().tolist()\n",
    "\n",
    "cnf_df = pd.DataFrame(cnf_matrix, index=classes, columns=classes)\n",
    "cnf_df = cnf_df[remaining_industries]\n",
    "cnf_df = cnf_df.loc[remaining_industries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10*PLT_SCALING_FACTOR, 8*PLT_SCALING_FACTOR))\n",
    "\n",
    "if NORMALIZE_CM:\n",
    "    normalized_cnf_df = cnf_df.astype('float') / cnf_df.sum(axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(normalized_cnf_df, annot=True, cmap=sns.color_palette(\"Blues\"), fmt='.2f')\n",
    "else:\n",
    "    sns.heatmap(cnf_df, annot=True, cmap=sns.color_palette(\"Blues\"), fmt='g')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
