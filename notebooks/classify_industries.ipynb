{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify Industries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOS \n",
    "\n",
    "- top 10 klassen oder so\n",
    "- preprocessing function einführen\n",
    "    - `\\n` weg\n",
    "    - andere unnütze Zeichen wie `|` etc.\n",
    "- remove pos einführen (siehe `clustering_whole_corpus`)\n",
    "    - vielleicht mit Language identifier?\n",
    "- ~~\"country\" als colum~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDEEN\n",
    "\n",
    "### Datensatzaufbereitung\n",
    "\n",
    "- Übersetzung der Websites in einheitliche Sprache (z.b. Englisch)\n",
    "- Andere Klassenlabels?\n",
    "    - Erst allgemeinere Klassen und dann in diesen Klassen feiner klassifizieren?\n",
    "        - Von: https://towardsdatascience.com/industrial-classification-of-websites-by-machine-learning-with-hands-on-python-3761b1b530f1\n",
    "            - Technology, Office, & Education products website (Class_1)\n",
    "            - Consumer products website (Class_2)\n",
    "            - Industrial Tools and Hardware products website (Class_3)\n",
    "    - seltene Klassenlables wegwerfen?\n",
    "\n",
    "### HTML Klassifizierung\n",
    "\n",
    "\n",
    "- Text zusammenfassen und dann klassifizieren? Dafür auch HTML-Tags verwenden?\n",
    "\n",
    "- HTML Struktur verwenden, um vorher **Boilerplate Content** von Main Content zu entfernen:\n",
    "    - Plain Text ist sehr noisy (viel unnötiges drin)\n",
    "- Bestimmten Wörtern/Tags höhere Gewichtungen geben\n",
    "    - Anchor Text (= klickbarer Text in einem Hyperlink)\n",
    "        - alleine zu wenig Inhalt (QI, S. 12)\n",
    "        - umliegende Wörter interessant! (QI, S. 12)\n",
    "        - auch für Nachbar-Seiten-Ansatz\n",
    "    - Title, Headers (QI, S. 12)\n",
    "        - auch für Nachbar-Seiten-Ansatz\n",
    "    - Keywords für Branchen\n",
    "        ```python3  \n",
    "        Class_1_keywords = ['Office', 'School', 'phone', 'Technology', 'Electronics', 'Cell', 'Business', 'Education', 'Classroom']\n",
    "        \n",
    "        Class_2_keywords = ['Restaurant', 'Hospitality', 'Tub', 'Drain', 'Pool', 'Filtration', 'Floor', 'Restroom', 'Consumer', 'Care', 'Bags', 'Disposables']\n",
    "        \n",
    "        Class_3_keywords = ['Pull', 'Lifts', 'Pneumatic', 'Emergency', 'Finishing', 'Hydraulic', 'Lockout', 'Towers', 'Drywall', 'Tools', 'Packaging', 'Measure', 'Tag ']\n",
    "        ```\n",
    "- NER mit **Tags** als weitere Tokens\n",
    "- Features von \"Nachbarseiten\" verwenden\n",
    "    - Hilfreich, da mehr Infos als \"nur\" Startseite\n",
    "    - Fragen: \n",
    "        - Was sind Nachbarseiten, wie definieren?\n",
    "            - Webgraph Webseiten?\n",
    "            - Weitere Seiten des Unternehmens?\n",
    "        - Wie viele Nachbarseiten?\n",
    "        - Wieviel von den Nachbarseiten verwenden?\n",
    "            - Ganze Seite?\n",
    "            - text, title, heading, Metadaten?\n",
    "    \n",
    "- *Weiteres*:\n",
    "    - Flat classification oder Hierarchical classification?\n",
    "        - Flat: parallele Klassen\n",
    "        - Hierarchical: hierarchische Klassen, bauen aufeinander auf\n",
    "    - Nur nach bestimmten Keywords filtern? (das geht jedoch mehr Richtung PLAIN-Textclassification)\n",
    "    - \"implicit links\": Seiten, die beide bei Suche von **Suchmaschine** erschienen sind und auf die beide der User geklickt hat (QI, S. 12) &rarr; nicht wirklich realisierbar\n",
    "\n",
    "\n",
    "## Paper / Repos\n",
    "\n",
    "- **Boilerplate Removal using a Neural Sequence Labeling Model** (2020): https://arxiv.org/pdf/2004.14294.pdf\n",
    "    - Verbesserung von **Web2Text** &rarr; basiert nicht auf teuren, handgemachten Feature Engineering\n",
    "    - <u>Hypothese</u>: \"Our hypothesis is that the **order** of text blocks in a web page **encodes important information** about their type, i.e. content or boilerplate, as the placement is determined by the authoring style\"\n",
    "- **Web2Text: Deep Structured Boilerplate Removal** (2018): https://arxiv.org/pdf/1801.02607.pdf\n",
    "- **Mozillas readability**: https://github.com/mozilla/readability\n",
    "- **Webpage Classification based on Compound of Using HTML Features & URLFeatures and Features of Sibling Pages** (2010): https://www.researchgate.net/publication/220419545_Webpage_Classification_based_on_Compound_of_Using_HTML_Features_URL_Features_and_Features_of_Sibling_Pages\n",
    "    - TODO\n",
    "- **Web Page Classification: Features and Algorithms** (2009): https://www.cs.ucf.edu/~dcm/Teaching/COT4810-Fall%202012/Literature/WebPageClassification.pdf\n",
    "    - S. 7: Using On-Page Features\n",
    "        - GOLUB, ARDO (2005): title, headings, metadata, main text\n",
    "    - TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "- Evaluation metric: **F1 Scores**\n",
    "- TF-IDF Vectorizer\n",
    "    - kein lowercase\n",
    "    - stop words werden entfernt\n",
    "    - keine max features\n",
    "- Top $n$ classes = most frequent classes\n",
    "- CLEAN HTML auch für Test Set (ansonsten unglaublich schlechte Accuracy und etwas sinnlos)\n",
    "\n",
    "\n",
    "#### Label: `group_representatives`\n",
    "\n",
    "| Experiment | KNN F1 (Precision) | LSVM F1 (Precision) |\n",
    "| ---------- |:-----:| ----:|\n",
    "| Plain Text (all samples) | **0.4295** (0.4774) | **0.6874** (0.7178) |\n",
    "| Plain HTML (all samples) | **0.1474** (0.1954) | **0.5109** (0.6139) |\n",
    "| | | |\n",
    "| Plain Text ([DE] all samples) | **0.5224** (0.5563) | **0.7209** (0.7476) |\n",
    "| Plain HTML ([DE] all samples) | **0.1515** (0.1915) | **0.5194** (0.6232) |\n",
    "| Plain Text ([DE, EN] all samples) | **0.472** (0.5258) | **0.7012** (0.7262) |\n",
    "| Plain HTML ([DE, EN] all samples) | **0.1466** (0.197) | **0.5166** (0.6211) |\n",
    "| | | |\n",
    "| Plain Text (all samples) (Top 10 classes) | **0.2059** (0.203) | **0.3024** (0.2721) |\n",
    "| Plain HTML (all samples) (Top 10 classes) | **0.0943** (0.0995) | **0.2376** (0.2323) |\n",
    "| Plain Text ([DE] all samples) (Top 10 classes) | **0.2212** (0.2036) | **0.3006** (0.2687) |\n",
    "| Plain HTML ([DE] all samples) (Top 10 classes) | **0.089** (0.0912) | **0.2231** (0.2164) |\n",
    "| | | |\n",
    "| Clean HTML (all samples)  | **0.0612** (0.0912) | **0.5086** (0.6424) |\n",
    "| Clean HTML ([DE] all samples)  | **0.0624** (0.0806) | **0.5579** (0.6607) |\n",
    "| Clean HTML ([DE, EN] all samples)  | **0.065** (0.088) | **0.5354** (0.645) |\n",
    "\n",
    "#### Label: `industry`\n",
    "\n",
    "| Experiment | KNN F1 (Precision) | LSVM F1 (Precision) |\n",
    "| ---------- |:-----:| ----:|\n",
    "| Plain Text (all samples)  | **0.3788** (0.4416) | **0.6218** (0.6423) |\n",
    "| Plain HTML (all samples)  | **0.1295** (0.1933) | **0.4589** (0.537) |\n",
    "| | | |\n",
    "| Plain Text ([DE] all samples)  | **0.4592** (0.5016) | **0.6446** (0.6591) |\n",
    "| Plain HTML ([DE] all samples)  | **0.1285** (0.1949) | **0.4574** (0.5382) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# sklearn classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# sklearn clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# sklearn general\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "import ujson as json\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from app.utils import clean_html_boilerplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR_PATH = \"../data/\"\n",
    "LANG = \"_DE\"\n",
    "\n",
    "\n",
    "INDUSTRIES_PATH_CSV = DATA_DIR_PATH + \"industries.csv\"\n",
    "TRAIN_PATH_CSV = DATA_DIR_PATH + \"train\" + LANG + \".csv\"\n",
    "TEST_PATH_CSV = DATA_DIR_PATH + \"test\" + LANG + \".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"text\" or \"html\"\n",
    "TEXT_COL = \"html\"\n",
    "\n",
    "# \"group_representative\", \"group_representative_label\", \"industry\", \"industry_label\" or \"group\"\n",
    "CLASS_COL = \"group_representative\"\n",
    "CLASS_NAMES = \"group_representative_label\"\n",
    "\n",
    "MAX_DOCUMENT_FREQUENCY = 1.\n",
    "MAX_FEATURES = 1000000\n",
    "LOWERCASE = False\n",
    "STOP_WORDS = get_stop_words(\"de\")\n",
    "\n",
    "# POS TAGGING\n",
    "POS_TAGGING = False\n",
    "POS_TAGS = [\"NOUN\"]\n",
    "   \n",
    "\n",
    "# USE TOP N CLASS LABELS\n",
    "USE_TOP_LABELS = False\n",
    "TOP_N_LABELS = 10\n",
    "\n",
    "\n",
    "# SUBSAMPLING\n",
    "SUBSAMPLING = False\n",
    "N_SAMPLES = 10000000\n",
    "USED_LANG = [\"DE\"] # \"ALL\" for no removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load train csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.45 s, sys: 554 ms, total: 5 s\n",
      "Wall time: 5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = pd.read_csv(TRAIN_PATH_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>industry</th>\n",
       "      <th>industry_label</th>\n",
       "      <th>group</th>\n",
       "      <th>group_representative</th>\n",
       "      <th>html</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>country</th>\n",
       "      <th>group_representative_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.kswtech.com</td>\n",
       "      <td>135</td>\n",
       "      <td>Mechanical or Industrial Engineering</td>\n",
       "      <td>cons, gov, man</td>\n",
       "      <td>135</td>\n",
       "      <td>&lt;html lang=\"de\"&gt;    &lt;head&gt;                    ...</td>\n",
       "      <td>KSW Elektro- und Industrieanlagenbau GmbH - KS...</td>\n",
       "      <td>xing</td>\n",
       "      <td>DE</td>\n",
       "      <td>Mechanical or Industrial Engineering</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      url  industry                        industry_label  \\\n",
       "0  http://www.kswtech.com       135  Mechanical or Industrial Engineering   \n",
       "\n",
       "            group  group_representative  \\\n",
       "0  cons, gov, man                   135   \n",
       "\n",
       "                                                html  \\\n",
       "0  <html lang=\"de\">    <head>                    ...   \n",
       "\n",
       "                                                text source country  \\\n",
       "0  KSW Elektro- und Industrieanlagenbau GmbH - KS...   xing      DE   \n",
       "\n",
       "             group_representative_label  \n",
       "0  Mechanical or Industrial Engineering  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19842, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(train.industry.value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some informations about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent countries:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DE    19842\n",
       "Name: country, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Most frequent countries:\\n\")\n",
    "train.country.value_counts().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average/mean share of actual/plain text of HTML: 19.0%\n"
     ]
    }
   ],
   "source": [
    "text_percentage = train.apply(lambda row: len(row.text)/len(row.html), axis=1)\n",
    "\n",
    "print(f\"Average/mean share of actual/plain text of HTML: {np.round(np.mean(text_percentage), decimals=2)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_representative_label (21): \n",
      "\n",
      "1. Automotive\t2. Construction\t3. Consumer Goods\t4. Financial Services\t5. Human Resources\t6. Information Technology and Services\t7. Insurance\t8. Legal Services\t9. Leisure, Travel & Tourism\t10. Logistics and Supply Chain\t11. Machinery\t12. Management Consulting\t13. Marketing and Advertising\t14. Mechanical or Industrial Engineering\t15. Media Production\t16. Medical Practice\t17. Real Estate\t18. Recreational Facilities and Services\t19. Renewables & Environment\t20. Telecommunications\t21. Wholesale\t"
     ]
    }
   ],
   "source": [
    "unique_classes = list(np.unique(train[CLASS_NAMES]))\n",
    "\n",
    "print(f\"{CLASS_NAMES} ({len(unique_classes)}): \\n\")\n",
    "for idx, i in enumerate(unique_classes):\n",
    "    print(str(idx+1)+\". \"+str(i), end=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUBSAMPLING\n",
    "\n",
    "- Only specific language (e.g. \"DE\")\n",
    "- Only $n$ samples (e.g. 1000)\n",
    "- Stratified sampling by industry col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of classes (sampled train): 21\n",
      "Equal to original train? True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(19842, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if SUBSAMPLING:\n",
    "    if USED_LANG[0] != \"ALL\":\n",
    "        train = train[train.country.isin(USED_LANG)]\n",
    "    if N_SAMPLES < train.shape[0]:\n",
    "        max_samples = N_SAMPLES\n",
    "    else:\n",
    "        max_samples = train.shape[0]\n",
    "    train = train.sample(n=max_samples, weights=CLASS_COL, random_state=1).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "unique_sampled_classes = len(train[CLASS_COL].unique())\n",
    "print(\"Count of classes (sampled train):\", unique_sampled_classes)\n",
    "print(\"Equal to original train?\", unique_sampled_classes == len(unique_classes))\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USE TOP N LABELS\n",
    "\n",
    "- only use top n classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using all labels.\n"
     ]
    }
   ],
   "source": [
    "if USE_TOP_LABELS:\n",
    "    top_n_classes = train[CLASS_COL].value_counts()[:TOP_N_LABELS].keys()\n",
    "    train = train[train[CLASS_COL].isin(top_n_classes)]\n",
    "    \n",
    "    unique_classes = list(np.unique(train[CLASS_NAMES]))\n",
    "\n",
    "    print(f\"{CLASS_NAMES} ({len(unique_classes)}): \\n\")\n",
    "    for idx, i in enumerate(unique_classes):\n",
    "        print(str(idx+1)+\". \"+str(i), end=\"\\t\")\n",
    "else:\n",
    "    print(\"Using all labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing (vectorizing, dimension reducing etc.)\n",
    "\n",
    "- ignore terms with a document frequency > MAX_DOCUMENT_FREQUENCY (`max_df` in TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of unique classes in train set: 21\n",
      "Count of unique languages in train set: 1\n",
      "CPU times: user 5.82 ms, sys: 705 µs, total: 6.53 ms\n",
      "Wall time: 5.83 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_labels = train[CLASS_COL].values\n",
    "unique_train_labels = list(np.unique(train[CLASS_COL]))\n",
    "print(\"Count of unique classes in train set:\", len(unique_train_labels))\n",
    "print(\"Count of unique languages in train set:\", len(np.unique(train[\"country\"].values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove tokens with POS-Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No POS TAGS are removed.\n",
      "\n",
      "CPU times: user 158 µs, sys: 46 µs, total: 204 µs\n",
      "Wall time: 172 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if POS_TAGGING:\n",
    "    train_text = remove_pos(train, pos_tags=POS_TAGS)\n",
    "else:\n",
    "    train_text = train[TEXT_COL]\n",
    "    print(\"No POS TAGS are removed.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 21s, sys: 369 ms, total: 1min 21s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=MAX_DOCUMENT_FREQUENCY,\n",
    "                             lowercase=LOWERCASE,\n",
    "                             max_features=MAX_FEATURES,\n",
    "                             stop_words=STOP_WORDS)\n",
    "\n",
    "\n",
    "vectorizer.fit(train_text)\n",
    "\n",
    "train_vector = vectorizer.transform(train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11 s, sys: 145 ms, total: 11.2 s\n",
      "Wall time: 11.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test = pd.read_csv(TEST_PATH_CSV)\n",
    "\n",
    "if SUBSAMPLING:\n",
    "    if USED_LANG[0] != \"ALL\":\n",
    "        test = test[test.country.isin(USED_LANG)]\n",
    "    test = test.sample(n=test.shape[0], weights=CLASS_COL, random_state=1).reset_index(drop=True)\n",
    "    \n",
    "\n",
    "test_vector = vectorizer.transform(test[TEXT_COL].values)\n",
    "test_labels = test[CLASS_COL].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors CLF \n",
      "-------------------------\n",
      "0.1683 \tPrecision\n",
      "0.1265 \tRecall\n",
      "0.13 \tF1\n",
      "\n",
      "CPU times: user 14.7 s, sys: 561 ms, total: 15.2 s\n",
      "Wall time: 15.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"K-Nearest Neighbors CLF\", \"\\n-------------------------\")\n",
    "# training\n",
    "clf = KNeighborsClassifier()\n",
    "clf.fit(train_vector, train_labels)\n",
    "\n",
    "# prediction\n",
    "train_preds = clf.predict(test_vector)\n",
    "\n",
    "# evaluation\n",
    "precision = precision_score(test_labels, train_preds, average=\"macro\", zero_division=0)\n",
    "recall = recall_score(test_labels, train_preds, average=\"macro\", zero_division=0)\n",
    "f1 = f1_score(test_labels, train_preds, average=\"macro\", zero_division=0)\n",
    "clf1_f1 = np.round(f1, decimals=4)\n",
    "clf1_precision = np.round(precision, decimals=4)\n",
    "\n",
    "print(np.round(precision, decimals=4), \"\\tPrecision\")\n",
    "print(np.round(recall, decimals=4), \"\\tRecall\")\n",
    "print(np.round(f1, decimals=4), \"\\tF1\")\n",
    "print()\n",
    "\n",
    "clf_report = classification_report(test_labels, \n",
    "                                   train_preds, \n",
    "                                   target_names = np.unique(test[CLASS_NAMES]), \n",
    "                                   zero_division = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSVM CLF \n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"LSVM CLF\", \"\\n-------------------------\")\n",
    "# training\n",
    "clf = LinearSVC(C = 1)\n",
    "clf.fit(train_vector, train_labels)\n",
    "\n",
    "# prediction\n",
    "train_preds = clf.predict(test_vector)\n",
    "\n",
    "# evaluation\n",
    "precision = precision_score(test_labels, train_preds, average=\"macro\", zero_division=0)\n",
    "recall = recall_score(test_labels, train_preds, average=\"macro\", zero_division=0)\n",
    "f1 = f1_score(test_labels, train_preds, average=\"macro\", zero_division=0)\n",
    "clf2_f1 = np.round(f1, decimals=4)\n",
    "clf2_precision = np.round(precision, decimals=4)\n",
    "\n",
    "print(np.round(precision, decimals=4), \"\\tPrecision\")\n",
    "print(np.round(recall, decimals=4), \"\\tRecall\")\n",
    "print(np.round(f1, decimals=4), \"\\tF1\")\n",
    "print()\n",
    "\n",
    "clf_report = classification_report(test_labels, \n",
    "                                   train_preds, \n",
    "                                   target_names = np.unique(test[CLASS_NAMES]),\n",
    "                                   zero_division = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Classification Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = \"| \"\n",
    "\n",
    "if TEXT_COL == \"text\":\n",
    "    result += \"Plain Text\"\n",
    "else:\n",
    "    result += \"HTML\"\n",
    "    \n",
    "result += \" (\"\n",
    "    \n",
    "if SUBSAMPLING:\n",
    "    result += \"[\" + \", \".join(USED_LANG) + \"]\"\n",
    "    if N_SAMPLES < train.shape[0]:\n",
    "        result += f\" {N_SAMPLES} samples\"\n",
    "    else:\n",
    "        result += \" all samples\"\n",
    "else:\n",
    "    result += \"all samples\"\n",
    "    \n",
    "result += \") \"\n",
    "\n",
    "if USE_TOP_LABELS:\n",
    "    result += f\"(Top {TOP_N_LABELS} classes)\"\n",
    "        \n",
    "result += f\" | **{clf1_f1}** ({clf1_precision}) | **{clf2_f1}** ({clf2_precision}) |\"\n",
    "print(CLASS_COL)\n",
    "print()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: label und text names und so; allg. änderungen von oben hier ergänzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMALIZE_CM = True\n",
    "INDUSTRY_TRESHOLD = 250\n",
    "PLT_SCALING_FACTOR = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "filtered_train = train.groupby(CLASS_COL).filter(lambda x: len(x)>INDUSTRY_TRESHOLD)\n",
    "remaining_industries = filtered_train[CLASS_NAMES].drop_duplicates().tolist()\n",
    "\n",
    "\n",
    "cnf_matrix = confusion_matrix(test_labels, train_preds)\n",
    "\n",
    "classes = train[CLASS_COL].drop_duplicates().tolist()\n",
    "\n",
    "cnf_df = pd.DataFrame(cnf_matrix, index=classes, columns=classes)\n",
    "cnf_df = cnf_df[remaining_industries]\n",
    "cnf_df = cnf_df.loc[remaining_industries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10*PLT_SCALING_FACTOR, 8*PLT_SCALING_FACTOR))\n",
    "\n",
    "if NORMALIZE_CM:\n",
    "    normalized_cnf_df = cnf_df.astype('float') / cnf_df.sum(axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(normalized_cnf_df, annot=True, cmap=sns.color_palette(\"Blues\"), fmt='.2f')\n",
    "else:\n",
    "    sns.heatmap(cnf_df, annot=True, cmap=sns.color_palette(\"Blues\"), fmt='g')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
