{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify Industries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOS \n",
    "\n",
    "- top 10 klassen oder so\n",
    "- preprocessing function einführen\n",
    "    - `\\n` weg\n",
    "    - andere unnütze Zeichen wie `|` etc.\n",
    "- remove pos einführen (siehe `clustering_whole_corpus`)\n",
    "    - vielleicht mit Language identifier?\n",
    "- ~~\"country\" als colum~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDEEN\n",
    "\n",
    "### Datensatzaufbereitung\n",
    "\n",
    "- Übersetzung der Websites in einheitliche Sprache (z.b. Englisch)\n",
    "- Andere Klassenlabels?\n",
    "    - Erst allgemeinere Klassen und dann in diesen Klassen feiner klassifizieren?\n",
    "        - Von: https://towardsdatascience.com/industrial-classification-of-websites-by-machine-learning-with-hands-on-python-3761b1b530f1\n",
    "            - Technology, Office, & Education products website (Class_1)\n",
    "            - Consumer products website (Class_2)\n",
    "            - Industrial Tools and Hardware products website (Class_3)\n",
    "    - seltene Klassenlables wegwerfen?\n",
    "\n",
    "### HTML Klassifizierung\n",
    "\n",
    "\n",
    "- Text zusammenfassen und dann klassifizieren? Dafür auch HTML-Tags verwenden?\n",
    "\n",
    "- HTML Struktur verwenden, um vorher **Boilerplate Content** von Main Content zu entfernen:\n",
    "    - Plain Text ist sehr noisy (viel unnötiges drin)\n",
    "- Bestimmten Wörtern/Tags höhere Gewichtungen geben\n",
    "    - Anchor Text (= klickbarer Text in einem Hyperlink)\n",
    "        - alleine zu wenig Inhalt (QI, S. 12)\n",
    "        - umliegende Wörter interessant! (QI, S. 12)\n",
    "        - auch für Nachbar-Seiten-Ansatz\n",
    "    - Title, Headers (QI, S. 12)\n",
    "        - auch für Nachbar-Seiten-Ansatz\n",
    "    - Keywords für Branchen\n",
    "        ```python3  \n",
    "        Class_1_keywords = ['Office', 'School', 'phone', 'Technology', 'Electronics', 'Cell', 'Business', 'Education', 'Classroom']\n",
    "        \n",
    "        Class_2_keywords = ['Restaurant', 'Hospitality', 'Tub', 'Drain', 'Pool', 'Filtration', 'Floor', 'Restroom', 'Consumer', 'Care', 'Bags', 'Disposables']\n",
    "        \n",
    "        Class_3_keywords = ['Pull', 'Lifts', 'Pneumatic', 'Emergency', 'Finishing', 'Hydraulic', 'Lockout', 'Towers', 'Drywall', 'Tools', 'Packaging', 'Measure', 'Tag ']\n",
    "        ```\n",
    "- NER mit **Tags** als weitere Tokens\n",
    "- Features von \"Nachbarseiten\" verwenden\n",
    "    - Hilfreich, da mehr Infos als \"nur\" Startseite\n",
    "    - Fragen: \n",
    "        - Was sind Nachbarseiten, wie definieren?\n",
    "            - Webgraph Webseiten?\n",
    "            - Weitere Seiten des Unternehmens?\n",
    "        - Wie viele Nachbarseiten?\n",
    "        - Wieviel von den Nachbarseiten verwenden?\n",
    "            - Ganze Seite?\n",
    "            - text, title, heading, Metadaten?\n",
    "    \n",
    "- *Weiteres*:\n",
    "    - Flat classification oder Hierarchical classification?\n",
    "        - Flat: parallele Klassen\n",
    "        - Hierarchical: hierarchische Klassen, bauen aufeinander auf\n",
    "    - Nur nach bestimmten Keywords filtern? (das geht jedoch mehr Richtung PLAIN-Textclassification)\n",
    "    - \"implicit links\": Seiten, die beide bei Suche von **Suchmaschine** erschienen sind und auf die beide der User geklickt hat (QI, S. 12) &rarr; nicht wirklich realisierbar\n",
    "\n",
    "\n",
    "## Paper / Repos\n",
    "\n",
    "- **Boilerplate Removal using a Neural Sequence Labeling Model** (2020): https://arxiv.org/pdf/2004.14294.pdf\n",
    "    - Verbesserung von **Web2Text** &rarr; basiert nicht auf teuren, handgemachten Feature Engineering\n",
    "    - <u>Hypothese</u>: \"Our hypothesis is that the **order** of text blocks in a web page **encodes important information** about their type, i.e. content or boilerplate, as the placement is determined by the authoring style\"\n",
    "- **Web2Text: Deep Structured Boilerplate Removal** (2018): https://arxiv.org/pdf/1801.02607.pdf\n",
    "- **Mozillas readability**: https://github.com/mozilla/readability\n",
    "- **Webpage Classification based on Compound of Using HTML Features & URLFeatures and Features of Sibling Pages** (2010): https://www.researchgate.net/publication/220419545_Webpage_Classification_based_on_Compound_of_Using_HTML_Features_URL_Features_and_Features_of_Sibling_Pages\n",
    "    - TODO\n",
    "- **Web Page Classification: Features and Algorithms** (2009): https://www.cs.ucf.edu/~dcm/Teaching/COT4810-Fall%202012/Literature/WebPageClassification.pdf\n",
    "    - S. 7: Using On-Page Features\n",
    "        - GOLUB, ARDO (2005): title, headings, metadata, main text\n",
    "    - TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "- Evaluation metric: **F1 Scores**\n",
    "- TF-IDF Vectorizer\n",
    "    - kein lowercase\n",
    "    - stop words werden entfernt\n",
    "    - keine max features\n",
    "- Top $n$ classes = most frequent classes\n",
    "- CLEAN HTML auch für Test Set (ansonsten unglaublich schlechte Accuracy und etwas sinnlos)\n",
    "\n",
    "\n",
    "#### Label: `group_representatives`\n",
    "\n",
    "| Experiment | KNN F1 (Precision) | LSVM F1 (Precision) |\n",
    "| ---------- |:-----:| ----:|\n",
    "| Plain Text (all samples) | **0.4295** (0.4774) | **0.6874** (0.7178) |\n",
    "| Plain HTML (all samples) | **0.1474** (0.1954) | **0.5109** (0.6139) |\n",
    "| | | |\n",
    "| Plain Text ([DE] all samples) | **0.5224** (0.5563) | **0.7209** (0.7476) |\n",
    "| Plain HTML ([DE] all samples) | **0.1515** (0.1915) | **0.5194** (0.6232) |\n",
    "| Plain Text ([DE, EN] all samples) | **0.472** (0.5258) | **0.7012** (0.7262) |\n",
    "| Plain HTML ([DE, EN] all samples) | **0.1466** (0.197) | **0.5166** (0.6211) |\n",
    "| | | |\n",
    "| Plain Text (all samples) (Top 10 classes) | **0.2059** (0.203) | **0.3024** (0.2721) |\n",
    "| Plain HTML (all samples) (Top 10 classes) | **0.0943** (0.0995) | **0.2376** (0.2323) |\n",
    "| Plain Text ([DE] all samples) (Top 10 classes) | **0.2212** (0.2036) | **0.3006** (0.2687) |\n",
    "| Plain HTML ([DE] all samples) (Top 10 classes) | **0.089** (0.0912) | **0.2231** (0.2164) |\n",
    "| | | |\n",
    "| Clean HTML (all samples)  | **0.0612** (0.0912) | **0.5086** (0.6424) |\n",
    "| Clean HTML ([DE] all samples)  | **0.0624** (0.0806) | **0.5579** (0.6607) |\n",
    "| Clean HTML ([DE, EN] all samples)  | **0.065** (0.088) | **0.5354** (0.645) |\n",
    "\n",
    "\n",
    "#### Label: `industry`\n",
    "\n",
    "| Experiment | KNN F1 (Precision) | LSVM F1 (Precision) |\n",
    "| ---------- |:-----:| ----:|\n",
    "| Plain Text (all samples)  | **0.3788** (0.4416) | **0.6218** (0.6423) |\n",
    "| Plain HTML (all samples)  | **0.1295** (0.1933) | **0.4589** (0.537) |\n",
    "| | | |\n",
    "| Plain Text ([DE] all samples)  | **0.4592** (0.5016) | **0.6446** (0.6591) |\n",
    "| Plain HTML ([DE] all samples)  | **0.1285** (0.1949) | **0.4574** (0.5382) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import justext\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# sklearn classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# sklearn clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# sklearn general\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "import ujson as json\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from app.utils import remove_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR_PATH = \"../data/\"\n",
    "TRAIN_PATH_CSV = DATA_DIR_PATH + \"train.csv\"\n",
    "TEST_PATH_CSV = DATA_DIR_PATH + \"test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"text\" or \"html\"\n",
    "TEXT_COL = \"html\"\n",
    "\n",
    "# \"group_representative\", \"group_representative_label\", \"industry\", \"industry_label\" or \"group\"\n",
    "CLASS_COL = \"industry\"\n",
    "CLASS_NAMES = \"industry_label\"\n",
    "\n",
    "MAX_DOCUMENT_FREQUENCY = 1.\n",
    "MAX_FEATURES = 1000000\n",
    "LOWERCASE = False\n",
    "STOP_WORDS = get_stop_words(\"de\")\n",
    "\n",
    "# POS TAGGING\n",
    "POS_TAGGING = False\n",
    "POS_TAGS = [\"NOUN\"]\n",
    "\n",
    "# HTML boilerplate removal\n",
    "if TEXT_COL == \"html\":\n",
    "    USE_CLEAN_HTML = False\n",
    "\n",
    "# USE TOP N CLASS LABELS\n",
    "USE_TOP_LABELS = False\n",
    "TOP_N_LABELS = 10\n",
    "\n",
    "\n",
    "# SUBSAMPLING\n",
    "SUBSAMPLING = True\n",
    "N_SAMPLES = 10000000\n",
    "USED_LANG = [\"DE\"] # \"ALL\" for no removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load train csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.6 s, sys: 6.67 s, total: 28.3 s\n",
      "Wall time: 32.1 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>industry</th>\n",
       "      <th>industry_label</th>\n",
       "      <th>group</th>\n",
       "      <th>group_representative</th>\n",
       "      <th>html</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>country</th>\n",
       "      <th>group_representative_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.i-q-i.net</td>\n",
       "      <td>11</td>\n",
       "      <td>Management Consulting</td>\n",
       "      <td>corp, consulting</td>\n",
       "      <td>11</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n\\n&lt;!--// OPEN HTML //--&gt;\\n&lt;ht...</td>\n",
       "      <td>iq! Management-Consulting &amp; Implementation Par...</td>\n",
       "      <td>xing</td>\n",
       "      <td>EN</td>\n",
       "      <td>Motion Pictures and Film</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.plentyleads.de</td>\n",
       "      <td>113</td>\n",
       "      <td>Online Media</td>\n",
       "      <td>med</td>\n",
       "      <td>126</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;&lt;html lang=\"de-DE\"&gt;&lt;head&gt;&lt;meta ...</td>\n",
       "      <td>plentyLeads - Online-Marketing für kleine und ...</td>\n",
       "      <td>xing</td>\n",
       "      <td>DE</td>\n",
       "      <td>Civic &amp; Social Organization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.fp-lims.com</td>\n",
       "      <td>4</td>\n",
       "      <td>Computer Software</td>\n",
       "      <td>tech</td>\n",
       "      <td>96</td>\n",
       "      <td>&lt;!DOCTYPE html&gt;\\n&lt;html lang=\"de-DE\" class=\"htm...</td>\n",
       "      <td>Laborsoftware - FP-LIMS - Qualitätskontrolle L...</td>\n",
       "      <td>xing</td>\n",
       "      <td>DE</td>\n",
       "      <td>Law Practice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.prosoft.net</td>\n",
       "      <td>4</td>\n",
       "      <td>Computer Software</td>\n",
       "      <td>tech</td>\n",
       "      <td>96</td>\n",
       "      <td>&lt;!doctype html&gt;\\n&lt;html lang=\"de\"&gt;\\n&lt;head&gt;\\n   ...</td>\n",
       "      <td>Software für Zeitarbeit, Gebäudereinigung &amp; Ev...</td>\n",
       "      <td>xing</td>\n",
       "      <td>DE</td>\n",
       "      <td>Law Practice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.karl-lohoff.de</td>\n",
       "      <td>56</td>\n",
       "      <td>Mining &amp; Metals</td>\n",
       "      <td>man</td>\n",
       "      <td>55</td>\n",
       "      <td>&lt;!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Tr...</td>\n",
       "      <td>Karl-Lohoff Gmbh &amp; Co.KG - Neuenkirchen | Germ...</td>\n",
       "      <td>xing</td>\n",
       "      <td>DE</td>\n",
       "      <td>Apparel &amp; Fashion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         url  industry         industry_label  \\\n",
       "0       http://www.i-q-i.net        11  Management Consulting   \n",
       "1  http://www.plentyleads.de       113           Online Media   \n",
       "2     http://www.fp-lims.com         4      Computer Software   \n",
       "3     http://www.prosoft.net         4      Computer Software   \n",
       "4  http://www.karl-lohoff.de        56        Mining & Metals   \n",
       "\n",
       "              group  group_representative  \\\n",
       "0  corp, consulting                    11   \n",
       "1               med                   126   \n",
       "2              tech                    96   \n",
       "3              tech                    96   \n",
       "4               man                    55   \n",
       "\n",
       "                                                html  \\\n",
       "0  <!DOCTYPE html>\\n\\n<!--// OPEN HTML //-->\\n<ht...   \n",
       "1  <!DOCTYPE html><html lang=\"de-DE\"><head><meta ...   \n",
       "2  <!DOCTYPE html>\\n<html lang=\"de-DE\" class=\"htm...   \n",
       "3  <!doctype html>\\n<html lang=\"de\">\\n<head>\\n   ...   \n",
       "4  <!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Tr...   \n",
       "\n",
       "                                                text source country  \\\n",
       "0  iq! Management-Consulting & Implementation Par...   xing      EN   \n",
       "1  plentyLeads - Online-Marketing für kleine und ...   xing      DE   \n",
       "2  Laborsoftware - FP-LIMS - Qualitätskontrolle L...   xing      DE   \n",
       "3  Software für Zeitarbeit, Gebäudereinigung & Ev...   xing      DE   \n",
       "4  Karl-Lohoff Gmbh & Co.KG - Neuenkirchen | Germ...   xing      DE   \n",
       "\n",
       "    group_representative_label  \n",
       "0     Motion Pictures and Film  \n",
       "1  Civic & Social Organization  \n",
       "2                 Law Practice  \n",
       "3                 Law Practice  \n",
       "4            Apparel & Fashion  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "train = pd.read_csv(TRAIN_PATH_CSV)\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30292, 10)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some informations about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent countries:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DE    19794\n",
       "EN     8259\n",
       "NL      451\n",
       "FR      401\n",
       "ES      295\n",
       "Name: country, dtype: int64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Most frequent countries:\\n\")\n",
    "train.country.value_counts().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average/mean share of actual/plain text of HTML: 8.0%\n"
     ]
    }
   ],
   "source": [
    "text_percentage = train.apply(lambda row: len(row.text)/len(row.html), axis=1)\n",
    "\n",
    "print(f\"Average/mean share of actual/plain text of HTML: {np.round(np.mean(text_percentage), decimals=2)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "industry_label (30): \n",
      "\n",
      "1. Architecture & Planning\t2. Automotive\t3. Banking\t4. Computer Software\t5. Construction\t6. Electrical/Electronic Manufacturing\t7. Financial Services\t8. Food & Beverages\t9. Health, Wellness and Fitness\t10. Hospitality\t11. Human Resources\t12. Information Technology and Services\t13. Insurance\t14. Internet\t15. Legal Services\t16. Logistics and Supply Chain\t17. Machinery\t18. Management Consulting\t19. Marketing and Advertising\t20. Mechanical or Industrial Engineering\t21. Medical Devices\t22. Mining & Metals\t23. Oil & Energy\t24. Online Media\t25. Pharmaceuticals\t26. Real Estate\t27. Renewables & Environment\t28. Retail\t29. Telecommunications\t30. Wholesale\t"
     ]
    }
   ],
   "source": [
    "unique_classes = list(np.unique(train[CLASS_NAMES]))\n",
    "\n",
    "print(f\"{CLASS_NAMES} ({len(unique_classes)}): \\n\")\n",
    "for idx, i in enumerate(unique_classes):\n",
    "    print(str(idx+1)+\". \"+str(i), end=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUBSAMPLING\n",
    "\n",
    "- Only specific language (e.g. \"DE\")\n",
    "- Only $n$ samples (e.g. 1000)\n",
    "- Stratified sampling by industry col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of classes (sampled train): 30\n",
      "Equal to original train? True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(19794, 10)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if SUBSAMPLING:\n",
    "    if USED_LANG[0] != \"ALL\":\n",
    "        train = train[train.country.isin(USED_LANG)]\n",
    "    if N_SAMPLES < train.shape[0]:\n",
    "        max_samples = N_SAMPLES\n",
    "    else:\n",
    "        max_samples = train.shape[0]\n",
    "    train = train.sample(n=max_samples, weights=CLASS_COL, random_state=1).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "unique_sampled_classes = len(train[CLASS_COL].unique())\n",
    "print(\"Count of classes (sampled train):\", unique_sampled_classes)\n",
    "print(\"Equal to original train?\", unique_sampled_classes == len(unique_classes))\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USE TOP N LABELS\n",
    "\n",
    "- only use top n classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_TOP_LABELS:\n",
    "    top_n_classes = train[CLASS_COL].value_counts()[:TOP_N_LABELS].keys()\n",
    "    train = train[train[CLASS_COL].isin(top_n_classes)]\n",
    "    \n",
    "    unique_classes = list(np.unique(train[CLASS_NAMES]))\n",
    "\n",
    "    print(f\"{CLASS_NAMES} ({len(unique_classes)}): \\n\")\n",
    "    for idx, i in enumerate(unique_classes):\n",
    "        print(str(idx+1)+\". \"+str(i), end=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing (vectorizing, dimension reducing etc.)\n",
    "\n",
    "- ignore terms with a document frequency > MAX_DOCUMENT_FREQUENCY (`max_df` in TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: verbessern/erweitern\n",
    "def lang_checker(lang): \n",
    "    languages = {\"DE\" : \"German\"}\n",
    "    if lang in languages.keys():\n",
    "        return languages[lang]\n",
    "    else:\n",
    "        return \"English\"\n",
    "\n",
    "def clean_html(row):\n",
    "    \"\"\" TODO \"\"\"\n",
    "        \n",
    "    paragraphs = justext.justext(row[\"html\"], justext.get_stoplist(lang_checker(row[\"country\"])))\n",
    "    \n",
    "    cleaned_html = [paragraph.text for paragraph in paragraphs if not paragraph.is_boilerplate]\n",
    "    return \" \".join(cleaned_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of unique classes in train set: 30\n",
      "Count of unique languages in train set: 1\n",
      "CPU times: user 6.23 ms, sys: 695 µs, total: 6.93 ms\n",
      "Wall time: 6.13 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if TEXT_COL == \"html\":\n",
    "    POS_TAGGING = False\n",
    "    \n",
    "    if USE_CLEAN_HTML:\n",
    "        train[\"html\"] = train.apply(lambda row: clean_html(row), axis=1)\n",
    "        \n",
    "train_text_plain = train[TEXT_COL].values\n",
    "\n",
    "\n",
    "train_labels = train[CLASS_COL].values\n",
    "unique_train_labels = list(np.unique(train[CLASS_COL]))\n",
    "print(\"Count of unique classes in train set:\", len(unique_train_labels))\n",
    "print(\"Count of unique languages in train set:\", len(np.unique(train[\"country\"].values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove tokens with POS-Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No POS TAGS are removed.\n",
      "\n",
      "CPU times: user 2.96 ms, sys: 8.39 ms, total: 11.3 ms\n",
      "Wall time: 11.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if POS_TAGGING:\n",
    "    train_text = remove_pos(train, pos_tags=POS_TAGS)\n",
    "else:\n",
    "    train_text = train_text_plain\n",
    "    print(\"No POS TAGS are removed.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 13s, sys: 1.68 s, total: 4min 15s\n",
      "Wall time: 4min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=MAX_DOCUMENT_FREQUENCY,\n",
    "                             lowercase=LOWERCASE,\n",
    "                             max_features=MAX_FEATURES,\n",
    "                             stop_words=STOP_WORDS)\n",
    "\n",
    "\n",
    "vectorizer.fit(train_text)\n",
    "\n",
    "train_vector = vectorizer.transform(train_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36 s, sys: 729 ms, total: 36.7 s\n",
      "Wall time: 36.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test = pd.read_csv(TEST_PATH_CSV)\n",
    "\n",
    "if SUBSAMPLING:\n",
    "    if USED_LANG[0] != \"ALL\":\n",
    "        test = test[test.country.isin(USED_LANG)]\n",
    "    test = test.sample(n=test.shape[0], weights=CLASS_COL, random_state=1).reset_index(drop=True)\n",
    "    \n",
    "if USE_CLEAN_HTML:\n",
    "    test[\"html\"] = test.apply(lambda row: clean_html(row), axis=1)\n",
    "\n",
    "\n",
    "test_vector = vectorizer.transform(test[TEXT_COL].values)\n",
    "test_labels = test[CLASS_COL].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors CLF \n",
      "-------------------------\n",
      "0.1949 \tPrecision\n",
      "0.1181 \tRecall\n",
      "0.1285 \tF1\n",
      "\n",
      "CPU times: user 41.4 s, sys: 827 ms, total: 42.2 s\n",
      "Wall time: 42.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"K-Nearest Neighbors CLF\", \"\\n-------------------------\")\n",
    "# training\n",
    "clf = KNeighborsClassifier()\n",
    "clf.fit(train_vector, train_labels)\n",
    "\n",
    "# prediction\n",
    "train_preds = clf.predict(test_vector)\n",
    "\n",
    "# evaluation\n",
    "precision = precision_score(test_labels, train_preds, average=\"macro\", zero_division=0)\n",
    "recall = recall_score(test_labels, train_preds, average=\"macro\", zero_division=0)\n",
    "f1 = f1_score(test_labels, train_preds, average=\"macro\", zero_division=0)\n",
    "clf1_f1 = np.round(f1, decimals=4)\n",
    "clf1_precision = np.round(precision, decimals=4)\n",
    "\n",
    "print(np.round(precision, decimals=4), \"\\tPrecision\")\n",
    "print(np.round(recall, decimals=4), \"\\tRecall\")\n",
    "print(np.round(f1, decimals=4), \"\\tF1\")\n",
    "print()\n",
    "\n",
    "clf_report = classification_report(test_labels, \n",
    "                                   train_preds, \n",
    "                                   target_names = np.unique(test[CLASS_NAMES]), \n",
    "                                   zero_division = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSVM CLF \n",
      "-------------------------\n",
      "0.5382 \tPrecision\n",
      "0.4241 \tRecall\n",
      "0.4574 \tF1\n",
      "\n",
      "CPU times: user 1min 16s, sys: 120 ms, total: 1min 16s\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"LSVM CLF\", \"\\n-------------------------\")\n",
    "# training\n",
    "clf = LinearSVC(C = 1)\n",
    "clf.fit(train_vector, train_labels)\n",
    "\n",
    "# prediction\n",
    "train_preds = clf.predict(test_vector)\n",
    "\n",
    "# evaluation\n",
    "precision = precision_score(test_labels, train_preds, average=\"macro\", zero_division=0)\n",
    "recall = recall_score(test_labels, train_preds, average=\"macro\", zero_division=0)\n",
    "f1 = f1_score(test_labels, train_preds, average=\"macro\", zero_division=0)\n",
    "clf2_f1 = np.round(f1, decimals=4)\n",
    "clf2_precision = np.round(precision, decimals=4)\n",
    "\n",
    "print(np.round(precision, decimals=4), \"\\tPrecision\")\n",
    "print(np.round(recall, decimals=4), \"\\tRecall\")\n",
    "print(np.round(f1, decimals=4), \"\\tF1\")\n",
    "print()\n",
    "\n",
    "clf_report = classification_report(test_labels, \n",
    "                                   train_preds, \n",
    "                                   target_names = np.unique(test[CLASS_NAMES]),\n",
    "                                   zero_division = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Classification Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "industry\n",
      "\n",
      "| Plain HTML ([DE] all samples)  | **0.1285** (0.1949) | **0.4574** (0.5382) |\n"
     ]
    }
   ],
   "source": [
    "result = \"| \"\n",
    "\n",
    "if TEXT_COL == \"text\":\n",
    "    result += \"Plain Text\"\n",
    "else:\n",
    "    if USE_CLEAN_HTML:\n",
    "        result += \"Clean HTML\"\n",
    "    else:\n",
    "        result += \"Plain HTML\"\n",
    "    \n",
    "result += \" (\"\n",
    "    \n",
    "if SUBSAMPLING:\n",
    "    result += \"[\" + \", \".join(USED_LANG) + \"]\"\n",
    "    if N_SAMPLES < train.shape[0]:\n",
    "        result += f\" {N_SAMPLES} samples\"\n",
    "    else:\n",
    "        result += \" all samples\"\n",
    "else:\n",
    "    result += \"all samples\"\n",
    "    \n",
    "result += \") \"\n",
    "\n",
    "if USE_TOP_LABELS:\n",
    "    result += f\"(Top {TOP_N_LABELS} classes)\"\n",
    "        \n",
    "result += f\" | **{clf1_f1}** ({clf1_precision}) | **{clf2_f1}** ({clf2_precision}) |\"\n",
    "print(CLASS_COL)\n",
    "print()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: label und text names und so; allg. änderungen von oben hier ergänzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMALIZE_CM = True\n",
    "INDUSTRY_TRESHOLD = 250\n",
    "PLT_SCALING_FACTOR = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "filtered_train = train.groupby(CLASS_COL).filter(lambda x: len(x)>INDUSTRY_TRESHOLD)\n",
    "remaining_industries = filtered_train[CLASS_NAMES].drop_duplicates().tolist()\n",
    "\n",
    "\n",
    "cnf_matrix = confusion_matrix(test_labels, train_preds)\n",
    "\n",
    "classes = train[CLASS_COL].drop_duplicates().tolist()\n",
    "\n",
    "cnf_df = pd.DataFrame(cnf_matrix, index=classes, columns=classes)\n",
    "cnf_df = cnf_df[remaining_industries]\n",
    "cnf_df = cnf_df.loc[remaining_industries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10*PLT_SCALING_FACTOR, 8*PLT_SCALING_FACTOR))\n",
    "\n",
    "if NORMALIZE_CM:\n",
    "    normalized_cnf_df = cnf_df.astype('float') / cnf_df.sum(axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(normalized_cnf_df, annot=True, cmap=sns.color_palette(\"Blues\"), fmt='.2f')\n",
    "else:\n",
    "    sns.heatmap(cnf_df, annot=True, cmap=sns.color_palette(\"Blues\"), fmt='g')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
